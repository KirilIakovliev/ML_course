{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 767.0387573891497\n",
      "Epoch 1: 654.7200182975607\n",
      "Epoch 2: 593.8478164672852\n",
      "Epoch 3: 555.564276837288\n",
      "Epoch 4: 567.7827603279276\n",
      "Epoch 5: 539.376040324759\n",
      "Epoch 6: 515.8916279569585\n",
      "Epoch 7: 544.698980420701\n",
      "Epoch 8: 546.2159795233543\n",
      "Epoch 9: 496.74433015863946\n",
      "Epoch 10: 520.8186554279733\n",
      "Epoch 11: 546.86419015438\n",
      "Epoch 12: 477.54597486130734\n",
      "Epoch 13: 558.9010512981009\n",
      "Epoch 14: 484.2106445637155\n",
      "Epoch 15: 494.7110846336852\n",
      "Epoch 16: 504.16928694704745\n",
      "Epoch 17: 529.6609443177568\n",
      "Epoch 18: 478.3484826595225\n",
      "Epoch 19: 498.0356779707239\n",
      "Testing score f1: 0.8929463770901402\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from numpy import unique, array, vectorize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class SVMClassifier:\n",
    "\n",
    "    def __init__(self, train_data=None):\n",
    "        data, labels = train_data\n",
    "\n",
    "        labels = self._transform_labels(labels)\n",
    "        \n",
    "        data = self._flatten_input(data)\n",
    "        \n",
    "        self.train_data = (data, labels)\n",
    "\n",
    "        self._open_session()\n",
    "\n",
    "        self.assemble_graph()\n",
    "\n",
    "        if train_data:\n",
    "            self.train()     \n",
    "\n",
    "    def assemble_graph(self, learning_rate = 0.02):\n",
    "        self.X = tf.placeholder(name=\"input\", dtype=tf.float32, shape=(None, self.train_data[0].shape[1]))\n",
    "        self.y = tf.placeholder(name=\"label\", dtype=tf.float32, shape=(None,1))\n",
    "        self.W = tf.Variable(tf.zeros(shape=[self.train_data[0].shape[1], 1],\n",
    "                                     dtype = tf.float32))\n",
    "        self.b = tf.Variable(tf.zeros(shape=[1], dtype = tf.float32))\n",
    "        self.pred = tf.subtract(tf.matmul(self.X,self.W),self.b)\n",
    "        self.max = tf.maximum(0.0,tf.subtract(1.0,tf.multiply(self.y,self.pred)))\n",
    "        self.loss = tf.reduce_mean(self.max)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def train(self, epochs=20, minibatch_size=256):\n",
    "        batch = self._create_minibatches(minibatch_size)\n",
    "        writer = tf.summary.FileWriter('./tmp', tf.get_default_graph())\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for j in batch:\n",
    "                feed_dict = {self.X: j[0], self.y: np.array([j[1]]).T}\n",
    "                _, epoch_loss = self.sess.run([self.opt, self.loss], feed_dict = feed_dict)\n",
    "                loss += epoch_loss\n",
    "            print(f\"Epoch {i}: {loss/len(batch)}\")\n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = self._flatten_input(data)\n",
    "        feed_dict = {self.X:data}\n",
    "        prediction = self.sess.run(self.pred, feed_dict = feed_dict)\n",
    "        prediction = np.sign(prediction)\n",
    "        prediction[prediction == -1] = 0\n",
    "        return prediction\n",
    "        \n",
    "    def _create_minibatches(self, minibatch_size):\n",
    "        pos = 0\n",
    "\n",
    "        data, labels = self.train_data\n",
    "        n_samples = len(labels)\n",
    "\n",
    "        batches = []\n",
    "        while pos + minibatch_size < n_samples:\n",
    "            batches.append((data[pos:pos+minibatch_size,:], labels[pos:pos+minibatch_size]))\n",
    "            pos += minibatch_size\n",
    "\n",
    "        if pos < n_samples:\n",
    "            batches.append((data[pos:n_samples,:], labels[pos:n_samples]))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def _transform_labels(self, labels):\n",
    "        transformed = [-1 if i == 0 else 1 for i in labels]\n",
    "        transformed = np.array(transformed)\n",
    "        return transformed\n",
    "\n",
    "    def _flatten_input(self, data):\n",
    "        return np.reshape(data,newshape=[-1,784])\n",
    "\n",
    "    def _open_session(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    def mnist_to_binary(train_data, train_label, test_data, test_label):\n",
    "\n",
    "        binarized_labels = []\n",
    "        for labels in [train_label, test_label]:\n",
    "            remainder_2 = vectorize(lambda x: x%2)\n",
    "            binarized_labels.append(remainder_2(labels))\n",
    "\n",
    "        train_label, test_label = binarized_labels\n",
    "\n",
    "        return train_data, train_label, test_data, test_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ((train_data, train_labels),\n",
    "        (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_data, train_labels, test_data, test_labels = mnist_to_binary(train_data, train_labels, eval_data, eval_labels)\n",
    "\n",
    "    svm = SVMClassifier((train_data, train_labels))\n",
    "    print(\"Testing score f1: {}\".format(f1_score(test_labels, svm.predict(test_data))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.6115232728897257\n",
      "Epoch 1: 0.5695848541056856\n",
      "Epoch 2: 0.5544047918725521\n",
      "Epoch 3: 0.5427426272250236\n",
      "Epoch 4: 0.5379628802867646\n",
      "Epoch 5: 0.5336445318891647\n",
      "Epoch 6: 0.5315862609985027\n",
      "Epoch 7: 0.5284078258149167\n",
      "Epoch 8: 0.5259947237816263\n",
      "Epoch 9: 0.5247536316831061\n",
      "Testing score f1: 0.9274546591619761\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from numpy import unique, array, vectorize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class CNNClassifier:\n",
    "\n",
    "    def __init__(self, train_data=None):\n",
    "        data, labels = train_data\n",
    "        \n",
    "        self.train_data = (data, labels)\n",
    "\n",
    "        self._open_session()\n",
    "\n",
    "        self.assemble_graph()\n",
    "\n",
    "        if train_data:\n",
    "            self.train()  \n",
    "\n",
    "\n",
    "    def assemble_graph(self, learning_rate = 0.02):\n",
    "        self.X = tf.placeholder(name='input', dtype = tf.float32, shape = [None, self.train_data[0].shape[1], self.train_data[0].shape[2], 1])\n",
    "        self.y = tf.placeholder(name='output',dtype = tf.float32, shape = [None])\n",
    "        conv1 = tf.layers.conv2d(inputs = self.X,filters = 3,\n",
    "                                 kernel_size = [5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size=[2, 2], strides=2)\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, filters = 3,\n",
    "                                kernel_size = [5, 5], padding = \"same\", activation = tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(inputs = conv2, pool_size = [2, 2], strides = 2)\n",
    "        pool2_reshape = tf.reshape(pool2, [-1, 7*7*3])\n",
    "        self.dense = tf.layers.dense(inputs = pool2_reshape, units = 4, activation = tf.nn.relu)\n",
    "        dense_logit1 = tf.layers.dense(inputs = self.dense, units = 1)\n",
    "        self.dense_logit2 = tf.nn.sigmoid(dense_logit1)\n",
    "        loss_ = tf.nn.sigmoid_cross_entropy_with_logits(labels = self.y, logits = tf.reshape(self.dense_logit2, [-1]))\n",
    "        self.loss = tf.reduce_mean(loss_)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def train(self, epochs = 10, minibatch_size = 256):\n",
    "        batch = self._create_minibatches(minibatch_size)\n",
    "        writer = tf.summary.FileWriter('./tmp', tf.get_default_graph())\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for j in batch:\n",
    "                feed_dict = {self.X: j[0][:,:,:,np.newaxis], self.y: np.array(j[1])}\n",
    "                _, epoch_loss = self.sess.run([self.opt, self.loss], feed_dict = feed_dict)\n",
    "                loss += epoch_loss\n",
    "            print(f\"Epoch {i}: {loss/len(batch)}\")\n",
    "        writer.close()\n",
    "        \n",
    "    def predict(self, data):\n",
    "        feed_dict = {self.X: data[:,:,:,np.newaxis]}\n",
    "        prediction = self.sess.run(self.dense_logit2, feed_dict = feed_dict)\n",
    "        predictions = [int(x) for x in prediction]\n",
    "        return predictions\n",
    "        \n",
    "    def _create_minibatches(self, minibatch_size):\n",
    "        pos = 0\n",
    "\n",
    "        data, labels = self.train_data\n",
    "        n_samples = len(labels)\n",
    "\n",
    "        batches = []\n",
    "        while pos + minibatch_size < n_samples:\n",
    "            batches.append((data[pos:pos+minibatch_size,:], labels[pos:pos+minibatch_size]))\n",
    "            pos += minibatch_size\n",
    "\n",
    "        if pos < n_samples:\n",
    "            batches.append((data[pos:n_samples,:], labels[pos:n_samples]))\n",
    "        return batches\n",
    "    \n",
    "    def _open_session(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "def mnist_to_binary(train_data, train_label, test_data, test_label):\n",
    "\n",
    "    binarized_labels = []\n",
    "    for labels in [train_label, test_label]:\n",
    "        remainder_2 = vectorize(lambda x: x%2)\n",
    "        binarized_labels.append(remainder_2(labels))\n",
    "\n",
    "    train_label, test_label = binarized_labels\n",
    "\n",
    "    return train_data, train_label, test_data, test_label\n",
    "\n",
    "((train_data, train_labels),\n",
    "        (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = mnist_to_binary(train_data, train_labels, eval_data, eval_labels)\n",
    "\n",
    "cnn = CNNClassifier((train_data, train_labels))\n",
    "print(\"Testing score f1: {}\".format(f1_score(test_labels, cnn.predict(test_data))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
